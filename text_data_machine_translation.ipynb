{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Note: Training for just 40 epochs, won't be performing hyperparameter tuning, and or any metric calculation (perplexity or BLEU score) on the out-of-distribution datasets (val and test). \n",
        "\n",
        "Overall, the goal is to just to transform text data into a form that works with our neural networks and to train a simple Encoder-Decoder model for German to English translation."
      ],
      "metadata": {
        "id": "D-MBUm1d3JcY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfo6DBbbFZni"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bXDugENCys2",
        "outputId": "cea477d3-09a5-4e63-c997-8b9e04d3d1ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import Counter # counter from python standard library\n",
        "import spacy                    # tokenization step \n",
        "\n",
        "import torchtext\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "import io\n",
        "import random\n",
        "\n",
        "# needed the following to fix a bash execution error within Colab\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "# let's make sure we have a GPU available\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5q5zfy6XT9w"
      },
      "source": [
        "In order to parse the sentence data we will be working with, we need to download the German and English dependencies from `spacy`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIf0UJVoGBxp"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlPiMz1rFcN3"
      },
      "source": [
        "# Download German / English Translation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQgWsRfEKVtX",
        "outputId": "07c76873-3879-43f0-8123-6cddf140df6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 637k/637k [00:00<00:00, 82.0MB/s]\n",
            "100%|██████████| 569k/569k [00:00<00:00, 68.9MB/s]\n",
            "100%|██████████| 24.7k/24.7k [00:00<00:00, 4.99MB/s]\n",
            "100%|██████████| 21.6k/21.6k [00:00<00:00, 8.49MB/s]\n",
            "100%|██████████| 22.9k/22.9k [00:00<00:00, 6.93MB/s]\n",
            "100%|██████████| 21.1k/21.1k [00:00<00:00, 9.39MB/s]\n"
          ]
        }
      ],
      "source": [
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
        "\n",
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U9T0s0PYWPv"
      },
      "source": [
        "This downloads and extracts the text data within the `.data` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmUp30yMKfLk",
        "outputId": "2cf564dc-599e-406d-a548-7a80ec5c85c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_2016_flickr.de\ttest_2016_flickr.en.gz\ttrain.en     val.de.gz\n",
            "test_2016_flickr.de.gz\ttrain.de\t\ttrain.en.gz  val.en\n",
            "test_2016_flickr.en\ttrain.de.gz\t\tval.de\t     val.en.gz\n"
          ]
        }
      ],
      "source": [
        "!ls .data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyKmoBH9YbNA"
      },
      "source": [
        "Now, let's examine the first 10 sentences of both languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6S4kwdQYlur",
        "outputId": "f75c7ad7-42bc-47f3-80d5-72dc89425a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "German : Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
            "English: Two young, White males are outside near many bushes.\n",
            "--------------------------------------------------\n",
            "German : Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
            "English: Several men in hard hats are operating a giant pulley system.\n",
            "--------------------------------------------------\n",
            "German : Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
            "English: A little girl climbing into a wooden playhouse.\n",
            "--------------------------------------------------\n",
            "German : Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
            "English: A man in a blue shirt is standing on a ladder cleaning a window.\n",
            "--------------------------------------------------\n",
            "German : Zwei Männer stehen am Herd und bereiten Essen zu.\n",
            "English: Two men are at the stove preparing food.\n",
            "--------------------------------------------------\n",
            "German : Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\n",
            "English: A man in green holds a guitar while the other man observes his shirt.\n",
            "--------------------------------------------------\n",
            "German : Ein Mann lächelt einen ausgestopften Löwen an.\n",
            "English: A man is smiling at a stuffed lion\n",
            "--------------------------------------------------\n",
            "German : Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.\n",
            "English: A trendy girl talking on her cellphone while gliding slowly down the street.\n",
            "--------------------------------------------------\n",
            "German : Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.\n",
            "English: A woman with a large purse is walking by a gate.\n",
            "--------------------------------------------------\n",
            "German : Jungen tanzen mitten in der Nacht auf Pfosten.\n",
            "English: Boys dancing on poles in the middle of the night.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "lines_en = open(\".data/train.en\", 'r', encoding=\"utf-8\").read().splitlines()\n",
        "lines_de = open(\".data/train.de\", 'r', encoding=\"utf-8\").read().splitlines()\n",
        "\n",
        "NUM_LINES = 10\n",
        "for i in range(NUM_LINES):\n",
        "  print(\"German :\", lines_de[i])\n",
        "  print(\"English:\", lines_en[i])\n",
        "  print(\"-\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk9WyergLD-0"
      },
      "source": [
        "Each setences is simply a Python string. So we need to convert this string into a tensor that works with our neural networks. The first step of this process is tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTGnSzlaFfx0"
      },
      "source": [
        "# Tokenizing our Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpGpr8_RLTFD"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from  torchtext.vocab import vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm') \n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OtqZMCmadA7"
      },
      "source": [
        "The first step is to iterate through our corpus and count the occurences of each token. Let's use a `Counter` from the `collections` package which is part of the Python Standard Library (it's similar to a Python dictionary). Let's test out the `Counter` on the training dataset for the English sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4QPPqtTLxMC"
      },
      "outputs": [],
      "source": [
        "counter = Counter()\n",
        "with io.open('.data/train.en', encoding='utf8') as f:\n",
        "  for string in f:\n",
        "    counter.update(en_tokenizer(string))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJwa9jTVbOLj"
      },
      "source": [
        "Let's see the number of unique tokens we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kegG-uCLMCwR",
        "outputId": "8ac4f114-7985-4dd6-ef61-3c56754086f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10834\n"
          ]
        }
      ],
      "source": [
        "print(len(counter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-jabNVYbTYO"
      },
      "source": [
        "Looks like we have 10834 unique words for our dataset. Now, let's print out the top 10 most frequent words in the Counter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6eU72OdbT0g",
        "outputId": "047f26fc-5f70-4a27-ed91-ce31aed59e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('a', 31707)\n",
            "('\\n', 29000)\n",
            "('.', 27623)\n",
            "('A', 17458)\n",
            "('in', 14847)\n",
            "('the', 9923)\n",
            "('on', 8019)\n",
            "('is', 7524)\n",
            "('and', 7378)\n",
            "('man', 7359)\n"
          ]
        }
      ],
      "source": [
        "sorted_counter = sorted(counter.items(), key=lambda kv: -kv[-1])\n",
        "for i in range(10):\n",
        "  print(sorted_counter[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6goQFt9cjAU"
      },
      "source": [
        "So, we can see the most frequent words found in our dataset. Note that the special character `\\n` is the second most frequent! At this point, we can convert our `Counter` object into a vocab. Each key in our `Counter` dictionary is now a token. Our Torch vocabulary will map a token to an index in our vocab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYx2LuN6Fjf4"
      },
      "source": [
        "# Building a Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlsfykfrMfHg"
      },
      "outputs": [],
      "source": [
        "en_vocab = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0nIgSf3dWMy"
      },
      "source": [
        "Now, let's see how to use our `vocab` object. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVp54jHkMx4l"
      },
      "source": [
        "# stoi: Convert a token to an index in our vocab.\n",
        "\n",
        "We can simply index into our `vocab` object with a token present in our training dataset. I'm guessing the word `cat` was in our training corpus:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP0txhrfMqSc",
        "outputId": "badae4f2-4129-4da1-d7f5-d6e9458b8ca1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1513"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "en_vocab[\"cat\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxM0FKq-dlQD"
      },
      "source": [
        "So, the token `cat` will always be mapped to the index `1513`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQKvqXVnM2P0"
      },
      "source": [
        "# itos: Convet an index in our vocab to its token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rjKk99udwHb"
      },
      "source": [
        "We can reverse this process as well using the `lookup_token()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xh6-Dn0vM6ar",
        "outputId": "69dd5e98-2085-4950-d90c-38d3bfe129ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "en_vocab.lookup_token(1513)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDXBDaZHd0Tv"
      },
      "source": [
        "And so, we can seamlessly move between the token representation to the index representation. Here's another example for the exclamation punctuation mark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Newfk7p2d3RF",
        "outputId": "52e54378-bb8e-48a6-9f5a-f9d0d8a5582c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2391\n"
          ]
        }
      ],
      "source": [
        "print(en_vocab['!'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMigzLAyd9di",
        "outputId": "305595b8-640a-452f-81a7-a82d1eaed9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\n"
          ]
        }
      ],
      "source": [
        "print(en_vocab.lookup_token(2391))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYqqlvH4d_qm"
      },
      "source": [
        "Now, let's check out the index in our vocab for the special tokens we passed to our `vocab` object from before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxTvwbxFNFPQ",
        "outputId": "1d050e95-f49f-4ebc-a4f3-fb33015e0e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "print(en_vocab['<unk>'])\n",
        "print(en_vocab['<pad>'])\n",
        "print(en_vocab['<bos>'])\n",
        "print(en_vocab['<eos>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F9cykZQeaaM"
      },
      "outputs": [],
      "source": [
        "en_vocab.set_default_index(en_vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VESsmv4eifX"
      },
      "source": [
        "Now, let's see what index we get for a couple of misspelled words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9S_xDa5elTX",
        "outputId": "66ba4ee7-62fd-4585-f517-b0e33ce49a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(en_vocab['fjflsj']) # some random string\n",
        "print(en_vocab['appel']) # apple misspelled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puD0pJ44esQD"
      },
      "source": [
        "They both get mapped to the unknown token! Let's wrap this all up in a function and build our vocab for both German and English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt6tfAZvN-yM"
      },
      "outputs": [],
      "source": [
        "def build_vocab(filepath, tokenizer):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for string_ in f:\n",
        "      counter.update(tokenizer(string_))\n",
        "  return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "de_vocab = build_vocab('.data/train.de', de_tokenizer)\n",
        "de_vocab.set_default_index(de_vocab['<unk>'])\n",
        "en_vocab = build_vocab('.data/train.en', en_tokenizer)\n",
        "en_vocab.set_default_index(en_vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcrCfQT1e94w"
      },
      "source": [
        "Let's check the size of our vocab for both languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T8szu39OH9x",
        "outputId": "a18a761c-3a47-4645-9d42-c2f408912752"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19215, 10838)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(de_vocab), len(en_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWlD7DBOFm4k"
      },
      "source": [
        "# Convert Sentences to Torch Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tPX_SRNfArg"
      },
      "source": [
        "At this stage, we can now map tokens to indices which are integers. We are one step closer to being able to pass text data into our neural networks. The following function just maps each token in each sentecne to their respective index for both vocabs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IULl6tgSOVPI"
      },
      "outputs": [],
      "source": [
        "def data_process(filepaths):\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
        "                            dtype=torch.long)\n",
        "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
        "                            dtype=torch.long)\n",
        "    data.append((de_tensor_, en_tensor_))\n",
        "  return data\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuRqYwqROjD-",
        "outputId": "77b11710-56ba-4598-b2c9-7038696b8adf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]),\n",
              " tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBu5NTsvfQju"
      },
      "source": [
        "As a sanity check, let's convert each index in the first example sentence back to its token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C31QQvsMfW39",
        "outputId": "facd7e4e-d17e-4a54-b09e-b502a5929e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche . \n",
            " Two young , White males are outside near many bushes . \n",
            " "
          ]
        }
      ],
      "source": [
        "de_sentence, en_sentence = train_data[0] # get the first training example\n",
        "for idx in de_sentence:\n",
        "  print(de_vocab.lookup_token(idx), end=' ')\n",
        "\n",
        "for idx in en_sentence:\n",
        "  print(en_vocab.lookup_token(idx), end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkxqMl8AfrcO"
      },
      "source": [
        "Now, we are ready to create our DataLoaders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve1kG6EUFpzy"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go8zIrswPBEg"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0)) #BOS Sentence EOS - German\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0)) #BOS Sentence EOS - English\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2UYpLk7FryR"
      },
      "source": [
        "# Explore Torchified Sentence Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PagpKIIwf5F7"
      },
      "source": [
        "Let's explore the data that's returned by our Dataloader. This data will be input to our Encoder-Decoder Neural Machine Translation Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPd191HfPZqa",
        "outputId": "7813e3f2-0404-45fc-ab5f-fa786751f350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([31, 128]) torch.Size([33, 128])\n"
          ]
        }
      ],
      "source": [
        "src_de, trg_en = next(iter(train_iter))\n",
        "print(src_de.shape, trg_en.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY8MKOXmgEA-"
      },
      "source": [
        "The size of our torch objects are: `length of sentence` $\\times$ `batch size`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJpN9_E9gMqK"
      },
      "source": [
        "We set the batch size to 128 and each sentence in a batch will have the same length. Note that the sentence size might `differ` between the languages. Let's print out our first German sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKrXW2FAPsFE",
        "outputId": "b4e479c4-a2ba-4355-8062-68f567d0ddb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([   2,   24, 1072, 1674, 2843,  732,   11,   68, 2587,   52,   53,   12,\n",
            "        2884, 1521,  961, 1277,   47, 5235,  282,   16,   17,    3,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1])\n"
          ]
        }
      ],
      "source": [
        "print(src_de[:, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0wjoVLOgZnQ"
      },
      "source": [
        "Let's print the values of our special tokens again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENj-0u90gdSD",
        "outputId": "a907a31f-e171-4d87-9b97-7583799c53ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "3\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(en_vocab['<bos>'])\n",
        "print(en_vocab['<eos>'])\n",
        "print(en_vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqSLcDwhgk7S"
      },
      "source": [
        "So, we can see our sentence starts with the `<bos>` token, ends with several `<pad>` tokens. The actual sentences itself ends with the `<eos`> token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX8lB0wvhGoR"
      },
      "source": [
        "Let's convert this German sentence to their tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnv5psgnP6vV",
        "outputId": "c7b38288-7fd6-4910-ea6c-a4a3ec17d839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos> Ein weiß gekleideter Baseballspieler rutscht in die Base , während der grau gekleidete Spieler ihn zu berühren versucht . \n",
            " <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> "
          ]
        }
      ],
      "source": [
        "for word in src_de[:,0]:\n",
        "  print(de_vocab.lookup_token(word), end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlZJ7Zz7hKEb"
      },
      "source": [
        "Since there is a one to one mapping between the German and English sentences, let's go ahead and print the first English sentence as well. It should be the corresponding English sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXKG2EZpQaKL",
        "outputId": "a5a6203b-0f41-480f-a098-bdb8dfef6bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos> A baseball player in white is sliding into the base while the player in gray is trying to tag him . \n",
            " <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> "
          ]
        }
      ],
      "source": [
        "for word in trg_en[:,0]:\n",
        "  print(en_vocab.lookup_token(word), end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_KgNg-rhapM"
      },
      "source": [
        "Finally, let's print the first 10 German sentences. Each sentence will be a column that starts with the index 2 (`<bos>` token) and is padded to be the same length. Confirm that each sentence has the `<eos>` token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6NCPe2wQNN3",
        "outputId": "ec76680d-e73f-418b-87b7-0b30691aee17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
            "        [   24,    71,   408,    24,    71,    24,    24,    24,    24,    24],\n",
            "        [ 1072,    72, 14316,   251,    72,  2843,   251,  5434,    31,    97],\n",
            "        [ 1674,    19,    36,    19,    11,   463,    11,  1192,   116,    98],\n",
            "        [ 2843,   984,    37,  1006,    32,    22,    32,   213,   489,    19],\n",
            "        [  732,   274,  1759,  3961,   278, 18241,  5252,   852,    11,    37],\n",
            "        [   11,    58,    47,    22,    34,   827,    36,  2398,    12,  6561],\n",
            "        [   68,   490,  2544,   283,    49,  3505,    32,  2241, 11398,    35],\n",
            "        [ 2587,  1400,    16,  3385,    58,    52,  5929,    68,    52,   117],\n",
            "        [   52,    52,    17,    36,  7355,   233,    33,   280,    36,    22],\n",
            "        [   53,    61,     3,    22,   320,    47,  1255,   681,    32,   391],\n",
            "        [   12,    64,     1, 17631,    16,    37,     9,    52,  2853,   295],\n",
            "        [ 2884, 14690,     1,    16,    17,  2587,   307,   852,  3913,  7962],\n",
            "        [ 1521,  6200,     1,    17,     3,    47,    16,   449,    37,    16],\n",
            "        [  961,  2059,     1,     3,     1,  5167,    17,  1375,    73,    17],\n",
            "        [ 1277,    42,     1,     1,     1,    52,     3,   593,   410,     3],\n",
            "        [   47,    16,     1,     1,     1,    53,     1,  3190,    16,     1],\n",
            "        [ 5235,    17,     1,     1,     1,    12,     1,    16,    17,     1],\n",
            "        [  282,     3,     1,     1,     1, 17435,     1,    17,     3,     1],\n",
            "        [   16,     1,     1,     1,     1,    55,     1,     3,     1,     1],\n",
            "        [   17,     1,     1,     1,     1,  7966,     1,     1,     1,     1],\n",
            "        [    3,     1,     1,     1,     1,  5210,     1,     1,     1,     1],\n",
            "        [    1,     1,     1,     1,     1,   463,     1,     1,     1,     1],\n",
            "        [    1,     1,     1,     1,     1,    16,     1,     1,     1,     1],\n",
            "        [    1,     1,     1,     1,     1,    17,     1,     1,     1,     1],\n",
            "        [    1,     1,     1,     1,     1,     3,     1,     1,     1,     1],\n",
            "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n"
          ]
        }
      ],
      "source": [
        "print(src_de[:,:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhATK2pZFviT"
      },
      "source": [
        "# Setting up our Neural Machine Translation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sgO0GKnhlu8"
      },
      "source": [
        "We are ready to setup our Neural Machine Translation model. We will be using an Encoder-Decoder model where both the components will be RNNs (in particular, [GRUs](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)). Remember, we are working with sequence data so each token will be input sequentially to our GRU units. Recall also that the weights of our GRU units will be reused for each token. At a high level: our model looks like this:\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZUAAAAtCAYAAABmtfaXAAAMQWlDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnluSkEBoAQSkhN4EkRpASggt9I4gKiEJEEqMgaBiRxYVXAsqFrChqyIKVkAsKGJnUex9saCirIsFu/ImBXTdV7433zd3/vvPmf+cOXfm3jsAqB3niES5qDoAecICcWywP31ccgqd9BRgAAVEMAq4cbj5ImZ0dDiAZaj9e3l3HSDS9oq9VOuf/f+1aPD4+VwAkGiI03n53DyIDwCAV3NF4gIAiFLebGqBSIphBVpiGCDEC6U4U46rpThdjvfIbOJjWRC3A6CkwuGIMwFQvQR5eiE3E2qo9kPsKOQJhACo0SH2ycubzIM4DWJraCOCWKrPSP9BJ/NvmunDmhxO5jCWz0VWlAIE+aJczvT/Mx3/u+TlSoZ8WMKqkiUOiZXOGebtZs7kMClWgbhPmB4ZBbEmxB8EPJk9xCglSxKSILdHDbj5LJgzoAOxI48TEAaxAcRBwtzIcAWfniEIYkMMVwg6TVDAjodYF+KF/PzAOIXNJvHkWIUvtDFDzGIq+LMcscyv1Nd9SU4CU6H/OovPVuhjqkVZ8UkQUyA2LxQkRkKsCrFDfk5cmMJmbFEWK3LIRiyJlcZvDnEsXxjsL9fHCjPEQbEK+7K8/KH5YpuyBOxIBd5XkBUfIs8P1s7lyOKHc8Eu8YXMhCEdfv648KG58PgBgfK5Y8/4woQ4hc4HUYF/rHwsThHlRivscVN+brCUN4XYJb8wTjEWTyyAC1Kuj2eICqLj5XHiRdmc0Gh5PPgyEA5YIADQgQTWdDAZZANBZ19TH7yT9wQBDhCDTMAH9gpmaESSrEcIr3GgCPwJER/kD4/zl/XyQSHkvw6z8qs9yJD1FspG5IAnEOeBMJAL7yWyUcJhb4ngMWQE//DOgZUL482FVdr/7/kh9jvDhEy4gpEMeaSrDVkSA4kBxBBiENEG18d9cC88HF79YHXCGbjH0Dy+2xOeELoIDwnXCN2EW5MExeKfoowA3VA/SJGL9B9zgVtCTVfcH/eG6lAZ18H1gT3uAv0wcV/o2RWyLEXc0qzQf9L+2wx+eBoKO7IjGSWPIPuRrX8eqWqr6jqsIs31j/mRx5o+nG/WcM/P/lk/ZJ8H27CfLbGF2H7sDHYCO4cdwZoAHWvFmrEO7KgUD6+ux7LVNeQtVhZPDtQR/MPf0JOVZjLfsc6x1/GLvK+AP036jgasyaLpYkFmVgGdCb8IfDpbyHUYRXdydHIGQPp9kb++3sTIvhuITsd3bv4fAHi3Dg4OHv7OhbYCsNcdbv9D3zlrBvx0KANw9hBXIi6Uc7j0QoBvCTW40/SAETAD1nA+TsANeAE/EAhCQRSIB8lgIow+C65zMZgKZoJ5oBSUg2VgFVgHNoItYAfYDfaBJnAEnACnwQVwCVwDd+Dq6QEvQD94Bz4jCEJCqAgN0UOMEQvEDnFCGIgPEoiEI7FIMpKGZCJCRILMROYj5UgFsg7ZjNQie5FDyAnkHNKF3EIeIL3Ia+QTiqEqqBZqiFqio1EGykTD0Hh0ApqJTkGL0BJ0CboGrUF3oY3oCfQCeg3tRl+gAxjAlDEdzASzxxgYC4vCUrAMTIzNxsqwSqwGq8da4HO+gnVjfdhHnIjTcDpuD1dwCJ6Ac/Ep+Gx8Mb4O34E34u34FfwB3o9/I1AJBgQ7gieBTRhHyCRMJZQSKgnbCAcJp+Be6iG8IxKJOkQrojvci8nEbOIM4mLiemID8Tixi/iIOEAikfRIdiRvUhSJQyoglZLWknaRWkmXST2kD0rKSsZKTkpBSilKQqVipUqlnUrHlC4rPVX6TFYnW5A9yVFkHnk6eSl5K7mFfJHcQ/5M0aBYUbwp8ZRsyjzKGko95RTlLuWNsrKyqbKHcoyyQHmu8hrlPcpnlR8of1TRVLFVYamkqkhUlqhsVzmuckvlDZVKtaT6UVOoBdQl1FrqSep96gdVmqqDKluVpzpHtUq1UfWy6ks1spqFGlNtolqRWqXafrWLan3qZHVLdZY6R322epX6IfUb6gMaNI0xGlEaeRqLNXZqnNN4pknStNQM1ORplmhu0Typ+YiG0cxoLBqXNp+2lXaK1qNF1LLSYmtla5Vr7dbq1OrX1tR20U7UnqZdpX1Uu1sH07HUYevk6izV2adzXefTCMMRzBH8EYtG1I+4POK97khdP12+bplug+413U96dL1AvRy95XpNevf0cX1b/Rj9qfob9E/p943UGuk1kjuybOS+kbcNUANbg1iDGQZbDDoMBgyNDIMNRYZrDU8a9hnpGPkZZRutNDpm1GtMM/YxFhivNG41fk7XpjPpufQ19HZ6v4mBSYiJxGSzSafJZ1Mr0wTTYtMG03tmFDOGWYbZSrM2s35zY/MI85nmdea3LcgWDIssi9UWZyzeW1pZJlkusGyyfGala8W2KrKqs7prTbX2tZ5iXWN91YZow7DJsVlvc8kWtXW1zbKtsr1oh9q52Qns1tt1jSKM8hglHFUz6oa9ij3TvtC+zv6Bg45DuEOxQ5PDy9Hmo1NGLx99ZvQ3R1fHXMetjnfGaI4JHVM8pmXMaydbJ65TldNVZ6pzkPMc52bnVy52LnyXDS43XWmuEa4LXNtcv7q5u4nd6t163c3d09yr3W8wtBjRjMWMsx4ED3+POR5HPD56unkWeO7z/MvL3ivHa6fXs7FWY/ljt4595G3qzfHe7N3tQ/dJ89nk0+1r4svxrfF96Gfmx/Pb5veUacPMZu5ivvR39Bf7H/R/z/JkzWIdD8ACggPKAjoDNQMTAtcF3g8yDcoMqgvqD3YNnhF8PIQQEhayPOQG25DNZdey+0PdQ2eFtoephMWFrQt7GG4bLg5viUAjQiNWRNyNtIgURjZFgSh21Iqoe9FW0VOiD8cQY6JjqmKexI6JnRl7Jo4WNyluZ9y7eP/4pfF3EqwTJAltiWqJqYm1ie+TApIqkrrHjR43a9yFZP1kQXJzCiklMWVbysD4wPGrxvekuqaWpl6fYDVh2oRzE/Un5k48OkltEmfS/jRCWlLazrQvnChODWcgnZ1end7PZXFXc1/w/Hgreb18b34F/2mGd0ZFxrNM78wVmb1ZvlmVWX0ClmCd4FV2SPbG7Pc5UTnbcwZzk3Ib8pTy0vIOCTWFOcL2yUaTp03uEtmJSkXdUzynrJrSLw4Tb8tH8ifkNxdowR/5Dom15BfJg0KfwqrCD1MTp+6fpjFNOK1juu30RdOfFgUV/TYDn8Gd0TbTZOa8mQ9mMWdtno3MTp/dNsdsTsmcnrnBc3fMo8zLmfd7sWNxRfHb+UnzW0oMS+aWPPol+Je6UtVScemNBV4LNi7EFwoWdi5yXrR20bcyXtn5csfyyvIvi7mLz/865tc1vw4uyVjSudRt6YZlxGXCZdeX+y7fUaFRUVTxaEXEisaV9JVlK9+umrTqXKVL5cbVlNWS1d1rwtc0rzVfu2ztl3VZ665V+Vc1VBtUL6p+v563/vIGvw31Gw03lm/8tEmw6ebm4M2NNZY1lVuIWwq3PNmauPXMb4zfarfpbyvf9nW7cHv3jtgd7bXutbU7DXYurUPrJHW9u1J3XdodsLu53r5+c4NOQ/kesEey5/netL3X94Xta9vP2F9/wOJA9UHawbJGpHF6Y39TVlN3c3Jz16HQQ20tXi0HDzsc3n7E5EjVUe2jS49RjpUcG2wtah04LjredyLzxKO2SW13To47ebU9pr3zVNips6eDTp88wzzTetb77JFznucOnWecb7rgdqGxw7Xj4O+uvx/sdOtsvOh+sfmSx6WWrrFdxy77Xj5xJeDK6avsqxeuRV7rup5w/eaN1BvdN3k3n93KvfXqduHtz3fm3iXcLbunfq/yvsH9mj9s/mjodus++iDgQcfDuId3HnEfvXic//hLT8kT6pPKp8ZPa585PTvSG9R76fn45z0vRC8+95X+qfFn9Uvrlwf+8vuro39cf88r8avB14vf6L3Z/tblbdtA9MD9d3nvPr8v+6D3YcdHxsczn5I+Pf089Qvpy5qvNl9bvoV9uzuYNzgo4og5sl8BDFY0IwOA19sBoCYDQIPnM8p4+flPVhD5mVWGwH/C8jOirLgBUA//32P64N/NDQD2bIXHL6ivlgpANBWAeA+AOjsP16GzmuxcKS1EeA7YFPs1PS8d/JsiP3P+EPfPLZCquoCf238BGyZ8eRFdvfkAAAA4ZVhJZk1NACoAAAAIAAGHaQAEAAAAAQAAABoAAAAAAAKgAgAEAAAAAQAAAZWgAwAEAAAAAQAAAC0AAAAA71y+egAADoZJREFUeAHtnX9sVeUZx99WShFU4I/JUDNat0ydEopGXEg2CiyauDBas2RgMlsWXNxka/lvUBNKFnCLJrTBqQkmtFuYE5PRskSXbNBiligjGSVolMjWy1Cm7A8KtnQMavd83t7n7vTX7b235x7ac58nee57znveX8/3fN/zvD/OvbfImYSFwDekoBvCKiwG5VwTG/4SAzviasISMWx+XI3L0a6/Sb5LOea1bIZAqAjUSGmDpqMw+EGoKFthYSGwyLg6iqv035awAC7kcmYUsvEh2l5SeuPsa6939RmeSVDXPTD36uXeS6UhYmxFhYeAvy/N7V2u/G4mLCY7n64efOfPbcbXEKhQHEIZVoQhYAgYAoaAIeARMKdiRDAEDAFDwBAIDQFzKqFBaQUZAoaAIWAImFMxDhgChoAhYAiEhoA5ldCgtIIMAUPAEDAE1KnsEygK4ZXYH9ktjwUCPywEvpaUlLwWi7tlRlQUAl+Li4v/xa32r8CWlpbec//997uHH344trd/z549A+fOnbsrtgYWlmFfWbBgwcBTTz0V2y+bHjp0yB07duzewrqtsbX2DixraGhwM2bE81sHp0+fdvv27fuimHmTWli0fPly19jYiO2xlDfeeAOnEkvbCtCoooULFw4IX2PrVK5cuYJTKcBbG0uTi7DqmWeecbNmzYqlgW+++SZOxdumy1+xNNSMMgQMAUPAEIgWAXMq0eJttRkChoAhEGsEzKnE+vaacYaAIWAIRIuAOZVo8bbaDAFDwBCINQLmVGJ9e804Q8AQMASiRcCcSrR4W22GgCFgCMQaAX2leNJGvvzyy27lypXurrvy81WQRCLhysrKJt3OqVTAod+3uPPnzoxq0qqqGrfgjvzaSt33LavMez2jjJsiEV1dXa65udkpr2pqalxlZWWqdRqfihjnINN042SfVtGv7m50rsi/HevbPefmeW5VdY276ZZ5ebfj048S7nBbq1u/aVve65oqFcDR9vZ219nZ6Z99Izmarp3krajgO5e5S67cDm2mglM5depU7hakyVlVVeVaWlrSpJielw4daHGfftQtv2UgP2YQ1AjMoe7zHyciqGnqVUFngVNLlixx27ZtcytWrPDnbW1tvrFNTU2utrZ2woZnmm7CgqZJgldf2D6MryePdrgnV5e7f7zflXcL4Kp3anmvaWpUwPOOQc6gPBeCHM3ku4Q4lKVLl07KELid6zM3tJlK0AKMKpNZBWGRjGzovPPmzXM9PT2ODs3xmTNn3Ny5c1PelHiEfIiec0za+fPn+zi9TnwcZHV1rVv8UOWYppw82unK76lwiQ9O+Ov3LVsxLB3XwbdM/mgpOFocL57R3r/PnXFfuG3RsHI44cFw+bOLbvbNc92dUidCeqTvsx4farw/mcYfdBZGcfX19Skr4CZ8pSOfOHHCXbx40Z/raI/RIlgrZ0mfLt2iRYtcWZLLqUpicDCSr00/q/UP+4YX21LWTcS/IMfIpLzkeDyOKwdJozJWPfCYGZTyPN8zfm1LmCHc2rx5s2OQAx9VOIaPDIgI4WTwup4TIkeOHPEDJs5JD1+Vv1zX57FynGcucfCWtAh9Qq/7iAw+ijNIk3USOiuGA8quXbv8shiF0MDq6mo/Cuzo6PChekNCPSatnmPohQsXXHd3t3cqXLtO0iT17hVdG1X9DU+sdDufrnKMCPfsqHOcI72Xelx91VJ3sLXJvf2nA3Jc4Z2CxjMLCcaTR/7VzqejLMrsTjoqrjECbJaHA9cIdUR4WMp5dlP1sDjSRyQtUg94V4RdHx2Rjtba2priFJxlFEingmtwDr5yzqgPPsJZeD1WOtrI7IYRHumoQ2c+Ybc/w/KqJF2HaI3ovAzzZJ0MJ3P0ULvPN5J/Dd+vTA1Mgvx7ZWe9a96ywecJxre37EpxnItbJb9ynDwq6eohXX31Uvfb3dvcu3/t1CxRhDTwgOiknw9wk4E4HAoKD3t9rhLPdkNQ9BzeIvAQIR7HoM/cDRuGsCddcGAFxznXPkA/0LJ8QRl+5GWmQt10MBQBDIBCcBIYRxzXy8vLPVD+4hgfAEtaQvQ6SqfUDXHaRBPJsDl5LEFuoo4imPvgqcHU6fpNjX4ms6am3j3+4Hwfz8Oe2Un9L1r8OaMxlgfoRMF4ZhbyN6nulcPdjs629Vdtvqz1P2l0G1eV+byMEum4ew4n/GyHep6Ua6vkYYH0Xrog+RP+OOIPekat6HHRhGiTKE8vjicl8IgOpMtXysW6uroU1+Ar/ISvrGVr56Nzkg/HQjmajvLogNoJyct1HgLXSRJS7xnRFtEe0TbRVtFO0dAkOMtmMHLvgyvckw3cKufYt4N3zGKG8U+u7fhxlR8cMYjZ8ZvO1OxYHcmttw9xWrkHl9URpauHetl3+Y7wOGJJSH2VouDcI9oiCt5dolmJcmisTHAVTqYTuMeACY6qwE/4CI8pgyW18YTrpEUoK1vJm1OhYSrBYzywnhOyTJAORC1jsqGsTWJrXVInW1yZFABrUUjzO9GcZMevO8Zd/qJA7bQjl7eCS1HageiQjBxVVj9Wm+qI5z8+kyqL63RapPsDaX5RkZ+R+Ag+5Fz3WzRd6lqGB/2Xe8H7xaRmmGtYsj45m5OMKZOwKak4lk9FJyU87FFGZTgGOiChju60cDiKI9m+fbu/zuiNuJFCR2fJTEeLXGfZNlfhd7/k97/ulfz/H2HkWtjQTKVWsqMJ0edEQxFdIqUwuMQSqg6U+mRGzawCGck/HA15+yR9kMuL5eURHy/5eJFEJXicrh7Sl99dodkyDhOnThZJ4nVJzTjfOAnnSXx9Ursk3DtOuqyj4dlY/JuoIHUSbD3A54kc00TlpbueN6cyXqV0vKDQqTE03yLr4QNSB9PTF3Ksq0LyMbWtTOY/IWFLUr+bjIskGPmgp+PiBFhLDq49a4fWRnEedE7Ek+fW2xa5dU8PH7mwlzOZ5YPS0hsH/tPft1uqOKj1Zxl+U9JXi4I7ckS0RbRNtEE0Z2G0RifDqcA9Qs7ZtxspjPBwOMxOmLHQGTkfS9jwD47s0o0Gx8ofjFu8eLF76623ElevXt0QjM/imF+MXSP6eDIPHg7smkRnJuMmHcCRcpk1qzy0eu0wZ6DxI0O/93HLaLxJpxzFKakEeU1cpvVo/onChV/68uAnZ//eIel+PlHaca5XSPwK0arkdfAG6zbRxcm4jAL4CN9GPhs5Z5+E61NZiqNuHJ2SESHCmrN6TkIAQwCP6ZtKSE5nUMr7SLQzR62QfOWizaJLRTlvEu0RjVSYvbAWrU6DpYWjcv71b1X59W2N/0Nrk+98NI5OyDnCSFD3VHAe3iHJa6GUO0fCnbKPEuzQPlOWH8U33ADeH4p25qiPST6eOttFwb1StEV00njDJ2YewRkynRingARHgvCVTowSTzqVYDqcErxmFMgxEnQwPiKLD37NVv6fgtlaZ47Kz+F+W7RVdKVomWi9aEJ00gLH2Et55dnNjuVUhFnGu8eO+JdL4BIOh6VVhA14PSavn80MDnqHFIw/dKDVOyWWX8mvMyH4rZKuHk2TbVh642yynBftzFHLJB84gzfPhzJRjE+IZiXKIWa9PAsRQvZC2GgPOhV9lgZ5OVZleh3Oo9SBBGfTwf7ANa2b42wk8pkKjWOkx4yFzVDdzKQDMipkjwXgWCZT4Rob/Mh4o0RNm8ewUcpOhF2+LhUEy2VNWDtqMF6PcR7d73f5vY8Fd5T7t7nWbK33IzxGgOyJzJFR4K23L/L7KOSrk/0XNuiPygY/rykyO0EYFXKNTdU7v7ZUOnG327hl11T4/kqlNG+oR9HQEEU5xMNfZ85r165NcZF4nc3QGenEvA0DXznWDhpMR+eGpzgaNvZZJlNuh9j0bIpqk8RoqBgG+YqjqHt2rx/M0DA4u2fH0J4cvOQtL/bxkI1bm/wLH4fFabBPpxyDe+yraHzw7TLyNDxRKRyd7/dqfEHyka4eTXMdwkapsz6seuEOPIVPcFSdiToR6tm7d29qE14HRMTDS56fvK1IX0dwGMpLnrMMrEjH9gPPXNLBXy2fa3CdfBrnC8rgo4g08iddxzdt2lTx/PPPZ5Al9yQ0Du+rhuZeUvY5ly1b9l9Zp35JcoZ24wOt2CgjnZde7+q7Lk460I4pc7jugblXL/deqpMGgXnY8pyMtH56/PjxmWEXPFXK27JlC29Ovif7KvfloU1flTJPNbd3DVu6ykM906ZIeaFlUGb/r0mD1+eh0WukzIP9/f3X5f9Ugs4lD7b5Ivk/lUcffZTjm4vzVYmVawgYAoaAIVB4CETqVFjHG/l2TeFBbhYbAoaAIRAdAlE/cyN1KrqOFx2cVpMhYAgYAoWNAPsjUUqkTiVKw6wuQ8AQMAQMgegRMKcSPeZWoyFgCBgCsUXAnEpsb60ZZggYAoZA9AiYU4kec6vREDAEDIHYImBOJba31gwzBAwBQyB6BPTLeoNnz57N+puT0Tc39xp7e3vNgeYO31TL+bncz6Jsv+k71YxI1x76o8jQ16HTJbRr0wEBfx/lt9zczJnx/L7uyZMnU/fBO5WBgYHP9u/f79AYC7byW0om0x+B/tOnT5cEfxF4+ps02oKSkpLe0bEWMw0RuEybH3nkkWnY9Kyb7H+e3F27du17kvXurLNPvwxHp1+TrcVjIPBLiescIz5WUfILxR/EyqDCNeawmL5ctDTmEHyCfbr8xYmPiLnRZl48EOgXMzrjYYpZUSAIvF0gdjrbZyiUO212GgKGgCEQAQLmVCIA2aowBAwBQ6BQEDCnUih32uw0BAwBQyACBMypRACyVWEIGAKGQKEgoBv1hWJv3uy80n95xlb590STIQTkD7pK5Mi+ZzE1CeHvy+6GjW7W7DlTs4URt+qfH77HHxZ+HnG1sazO//NjLC2L1ihex14XbZXTojb+Se/9adHSwmtkY+GZPKHF70iKP06YyhKkReB/IWYr8uyKH/QAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0YspJzfk2Ep",
        "outputId": "a09aebb2-5c5f-4ca4-dd95-69b8580d3395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding(10, 4)\n",
            "tensor([[-0.3741,  0.6554, -1.7362,  0.0513],\n",
            "        [ 1.5785,  1.5529,  0.9754,  0.3551],\n",
            "        [-1.9181,  1.2132, -0.4472,  0.6887],\n",
            "        [ 0.1751, -0.4901,  0.1189,  0.6863],\n",
            "        [-0.7948, -2.2631, -3.5633,  1.3887],\n",
            "        [-0.9918, -0.6075, -0.0085, -0.5732],\n",
            "        [-0.1582, -0.4766, -0.7730, -0.3657],\n",
            "        [ 0.2304,  0.4892,  0.0140,  0.4349],\n",
            "        [ 0.0244, -1.2378, -0.6552,  0.9723],\n",
            "        [-1.3659,  1.7206, -1.0083,  0.5567]])\n"
          ]
        }
      ],
      "source": [
        "embedding = nn.Embedding(10, 4)\n",
        "print(embedding)\n",
        "print(embedding.weight.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8F4qtmQlvAG"
      },
      "source": [
        "Notice that the embedding is simply a $10 \\times 4$ weight matrix. Each token will index into our embedding and return a row of this matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWmWGYL2k5XS"
      },
      "source": [
        "Suppose we want to transform the `<bos>` token (which has an index of 2 in our both of our vocabs). We can do perform the embedding as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5249ozohl9w_",
        "outputId": "01eb951f-7b3d-4055-d030-73be405c25e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-1.9181,  1.2132, -0.4472,  0.6887]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding(torch.tensor([[en_vocab['<bos>']]]))) ## equivalent to embedding(torch.tensor([[2]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNboAjf0mGcw"
      },
      "source": [
        "This is just the third row in our embedding matrix. Note the associated `grad_fn` with the embedding table. This means we can train embeddings! Through the training process, similar words can be hopefully be transformed to similar embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kth_Bbw6mZHb"
      },
      "source": [
        "## Setting up the Model\n",
        "Now, we are ready to build our encoder-decoder model. We need to first embed our indices to our representation space and process them through the GRUs. We will be use a bidirectional GRU for the encoder in order to process the source German sentence in both directions ([Sutskever et al. 2014](https://arxiv.org/abs/1409.3215) found this helpful). Doing so increases the depth of the encoder GRU by a factor of two.\n",
        "\n",
        "We will use:\n",
        "\n",
        "- an embedding space of size 128\n",
        "- a hidden state size of 256\n",
        "- 4-layer GRUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8WohHKuRAPL"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    SRC_EMB_SIZE = 128\n",
        "    TRG_EMB_SIZE = 128\n",
        "    H_SIZE = 256\n",
        "    LAYERS = 4\n",
        "\n",
        "    self.src_emb = nn.Embedding(len(de_vocab), 128) # embedding for german tokens\n",
        "    self.trg_emb = nn.Embedding(len(en_vocab), 128) # embedding for english tokens\n",
        "\n",
        "    self.encoder = nn.GRU(SRC_EMB_SIZE, H_SIZE, LAYERS//2, bidirectional=True) # bidirectional encoder\n",
        "    self.decoder = nn.GRU(TRG_EMB_SIZE, H_SIZE, LAYERS) # decoder\n",
        "    self.to_trg = nn.Linear(H_SIZE, len(en_vocab)) # transform the hidden states to a probability distribution\n",
        "\n",
        "  def forward(self, src_ids, trg_ids):\n",
        "    src_emb = self.src_emb(src_ids) # embed the german tokens\n",
        "    enc_output, enc_hidden = self.encoder(src_emb) # process through the bidirectional GRU\n",
        "\n",
        "\n",
        "    trg_emb = self.trg_emb(trg_ids) # embed the english tokens (just for training)\n",
        "    dec_output, dec_hidden = self.decoder(trg_emb, enc_hidden) # process target through decoder GRU, note the hidden state \n",
        "\n",
        "    logits = self.to_trg(dec_output) # convert each hidden state to a distribution over the english language\n",
        "    preds = F.log_softmax(logits, dim=2) # convert to a normalized dist.\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3afPIihnZCf"
      },
      "source": [
        "Let's perform a sanity check and make sure things work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65z9JWYsUDHA",
        "outputId": "9b1a02ed-db3f-4361-cb31-474457959d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 128]) torch.Size([31, 128])\n"
          ]
        }
      ],
      "source": [
        "model = Seq2Seq()\n",
        "src_de, trg_en = next(iter(train_iter))\n",
        "print(src_de.shape, trg_en.shape)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  out = model(src_de, trg_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X-1aSh1nc_d"
      },
      "source": [
        "Let's analyze the output shape of our network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oze_dFknUTDL",
        "outputId": "bea87866-f209-443c-bd5f-527c65badd99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([31, 128, 10838])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQBYUCSenf0O"
      },
      "source": [
        "We see `torch.Size([31, 128, 10838])`:\n",
        "\n",
        "- 31 is the output size of our English sentences\n",
        "- 128 is the batch size\n",
        "- each of the 31 output hidden states maps to a distribution over the English langauge (which had a size of 10838 tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGBN51A4FyAD"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGY8tl-Cns2S"
      },
      "source": [
        "Let's train this model. We will use the Adam optimizer. Importantly, we don't want the `<pad>` token to contribute to our loss since it's just for training purposes, so let's handle that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6fPxWaMUiBy"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "PAD_IDX = en_vocab['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCCCYMQSoEkX"
      },
      "source": [
        "Note that we will be using teacher forcing as described previously. For the input to the decoder, we chop off the final token (`trg[:-1]`). For the loss function, we chop off the first token of the target sentence (`trg[1:]`) to obtain our ground truth tokens. Recall that RNN models are prone to both gradient vanishing and exploding. Let's clip the gradients if they are too large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzZnpb11Uzie",
        "outputId": "7395ff9e-6bf0-419f-d2f4-b3883b4daeb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 4.926530113304239\n",
            "Epoch 1: 4.003010595422484\n",
            "Epoch 2: 3.384852639378955\n",
            "Epoch 3: 3.013778653964072\n",
            "Epoch 4: 2.7345615998238717\n",
            "Epoch 5: 2.5076538938782815\n",
            "Epoch 6: 2.3079120944775147\n",
            "Epoch 7: 2.127656368957217\n",
            "Epoch 8: 1.9613322502715997\n",
            "Epoch 9: 1.8055939700634993\n",
            "Epoch 10: 1.6560819637407815\n",
            "Epoch 11: 1.5185737315778691\n",
            "Epoch 12: 1.388288369262796\n",
            "Epoch 13: 1.2640819502296952\n",
            "Epoch 14: 1.1521452927904507\n",
            "Epoch 15: 1.0449961833491725\n",
            "Epoch 16: 0.9463206175140347\n",
            "Epoch 17: 0.8578489257375574\n",
            "Epoch 18: 0.7723013556476207\n",
            "Epoch 19: 0.6927754246190782\n",
            "Epoch 20: 0.6184131293044741\n",
            "Epoch 21: 0.5528884221541199\n",
            "Epoch 22: 0.4944252854926996\n",
            "Epoch 23: 0.4407538074491308\n",
            "Epoch 24: 0.39049183705304685\n",
            "Epoch 25: 0.3448183296272933\n",
            "Epoch 26: 0.30316996797591056\n",
            "Epoch 27: 0.2650890880087924\n",
            "Epoch 28: 0.23480227569914075\n",
            "Epoch 29: 0.2096506143850377\n",
            "Epoch 30: 0.18725315563479183\n",
            "Epoch 31: 0.17502348755162192\n",
            "Epoch 32: 0.16177015991200436\n",
            "Epoch 33: 0.14989684728930175\n",
            "Epoch 34: 0.1383056765020156\n",
            "Epoch 35: 0.12525089761234065\n",
            "Epoch 36: 0.11209428707539773\n",
            "Epoch 37: 0.0999004048200956\n",
            "Epoch 38: 0.09157855086652193\n",
            "Epoch 39: 0.08771323279727923\n"
          ]
        }
      ],
      "source": [
        "model = model.cuda()\n",
        "model.train()\n",
        "for epoch in range(40):\n",
        "  epoch_loss = 0\n",
        "  for _, (src, trg) in enumerate(train_iter):\n",
        "    src, trg = src.cuda(), trg.cuda()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #teacher-forcing\n",
        "    output = model(src, trg[:-1]) # removing the last token\n",
        "    output = output.view(-1, output.shape[-1])\n",
        "\n",
        "    loss = criterion(output, trg[1:].view(-1)) # removing the first token\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1) # grad clipping\n",
        "\n",
        "    optimizer.step() # update our weights\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  print(f\"Epoch {epoch}:\", epoch_loss / len(train_iter))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUEhL8a7F1ls"
      },
      "source": [
        "# Model Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsqtA68qsiHC"
      },
      "source": [
        "At this point, we are ready to take a never-before-seen German sentence and translate it. We will write a fucntion below that performs the prediction as we noted above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuUve_SNY-dD"
      },
      "outputs": [],
      "source": [
        "def greedy_sample(src, trg, idx):\n",
        "##########################################################################\n",
        "  # push batch to the GPU\n",
        "  src, trg = src.cuda(), trg.cuda()\n",
        "  model.eval()\n",
        "\n",
        "  # pull one sentence out using the idx parameter\n",
        "  sample = src[:, idx:idx+1] # choosing a sentence in German\n",
        "  gt = trg[:, idx:idx+1]     # correct English translation from the dataset\n",
        "    \n",
        "  # convert indices to tokens\n",
        "  input = [de_vocab.lookup_token(sample[i,0]) for i in range(len(sample))]\n",
        "  input = input[1:input.index(\"<eos>\")]\n",
        "  gt = [en_vocab.lookup_token(gt[i, 0]) for i in range(len(gt))]\n",
        "  gt = gt[1:gt.index(\"<eos>\")]\n",
        "  # print both the input german sentence and the ground truth\n",
        "  print(\"input:\", \" \".join(input))\n",
        "  print(\"gt   :\", \" \".join(gt))\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "  # perform model prediction (inference)\n",
        "  output = []\n",
        "  # embed source sentence\n",
        "  src_sen_emb = model.src_emb(sample)\n",
        "  # process through the encoder\n",
        "  enc_output, enc_hidden = model.encoder(src_sen_emb)\n",
        "  dec_hidden = enc_hidden # set the first hidden state of the decoder to the final hidden state of encoder\n",
        "\n",
        "  # start our sentence with the beginning of sequence token\n",
        "  out = de_vocab['<bos>']\n",
        "\n",
        "  # sample for 40 tokens\n",
        "  for i in range(40):\n",
        "    \n",
        "    # embed the current token (starts at <bos>)\n",
        "    trg_sen_emb = model.trg_emb(torch.tensor([[out]], device='cuda'))\n",
        "    dec_output, dec_hidden = model.decoder(trg_sen_emb, dec_hidden) #note the hidden state, starts as enc_hidden\n",
        "    preds = F.log_softmax(model.to_trg(dec_output), dim=2) # produce probabilities for each decoder hidden state\n",
        "    out = preds.argmax(2) # choosing the word with the highest probability\n",
        "    output.append(en_vocab.lookup_token(out.item())) # convert to token\n",
        "    \n",
        "  # print our prediction\n",
        "  if \"<eos>\" in output:\n",
        "    output = output[:output.index(\"<eos>\")]\n",
        "  print(\"pred:\", \" \".join(output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1bjC3pptGi1"
      },
      "source": [
        "Let's try out our trained Neural Machine Translation Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHk1-B0zZ0rY",
        "outputId": "4db70c61-e2ba-47fe-ed09-eb1eabe8a1d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([34, 128]) torch.Size([36, 128])\n"
          ]
        }
      ],
      "source": [
        "src, trg = next(iter(test_iter))\n",
        "print(src.shape, trg.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM5focw-Z-BD",
        "outputId": "16a9de62-88fe-442a-e32b-80bff8e39739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: Zwei Männer in Anzügen unter einem Regenschirm vor einem Graffiti . \n",
            "\n",
            "gt   : Two men in suits under an umbrella and in front of graffiti . \n",
            "\n",
            "pred: Two men in military clothing are outside in a front of truck . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "rnd_idx = random.randint(0,128)\n",
        "greedy_sample(src, trg, rnd_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VWeKAR_tMuO"
      },
      "source": [
        "So, we can see it's performing *some* translation, but maybe not the best. Let's try a few more samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqHFb9ITwYsh",
        "outputId": "48fce7bc-94df-458f-9ab1-4626e8ef6fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: Ein junges Paar sitzt auf dem Gehsteig und entspannt gemeinsam . \n",
            "\n",
            "gt   : A young couple sits on the sidewalk and relaxes together . \n",
            "\n",
            "pred: A young couple is sitting on the sidewalk wearing camping or clothing . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "rnd_idx = random.randint(0,128)\n",
        "greedy_sample(src, trg, rnd_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO7S619kPDXh"
      },
      "source": [
        "Note the unknown tokens in the next example. This means that the token didn't appear in our German training dataset. Thankfully, our default `<unk>` token handles this gracefully for us. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3L-DMvPtiSK",
        "outputId": "e9812728-a840-4011-c229-e44815630bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: Leute gehen auf einer befestigten <unk> Straße , umgeben von <unk> Händlern . \n",
            "\n",
            "gt   : People are walking on a paved slope surrounded by Chinese vendors . \n",
            "\n",
            "pred: People walk down a busy crowded street with many stone lights . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "rnd_idx = random.randint(0,128)\n",
        "greedy_sample(src, trg, rnd_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jdqz08OtlYQ",
        "outputId": "cab5e53d-96ab-4684-e804-0c7e9484c540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: Ein Mann in einem weißen Hemd blickt durch ein Fenster einer Metallkonstruktion . \n",
            "\n",
            "gt   : A man wearing a white shirt is looking out a window of a metal construction . \n",
            "\n",
            "pred: A man in a white shirt looks at a window through a marketplace . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "rnd_idx = random.randint(0,128)\n",
        "greedy_sample(src, trg, rnd_idx)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}